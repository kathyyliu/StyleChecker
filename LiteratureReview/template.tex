\documentclass[10pt,twocolumn]{article} 

\usepackage{oxycomps} % use the main oxycomps style file

\bibliography{references}

\pdfinfo{
    /Title (Python Style Checker to Teach Code Style Literature Review)
    /Author (Kathy Liu)
}

\title{Python Style Checker to Teach Code Style Literature Review}

\author{Kathy Liu}
\affiliation{Occidental College}
\email{kliu4@oxy.edu}

\begin{document}

\maketitle

\section{Problem Context}
Code style is often thought of as adherence to syntactic coding standards for a language, such as use of whitespace, line length, formatting, and more. 
However, the importance of code style is much more than mere aesthetics, as it also extends to "code smells" that signal a deeper need to refactor, such as dead or repeated code. 
Thus, code style is at the heart of human readable, quality code and is an essential skill for programmers. 
For novice programmers that are still learning correct logic and have yet to build context to understand the nuances of the programs they write, grasping a sense for code style can feel overabstract. 
Building the habit of coding with good style early on helps students break out of the mindset that code that merely produces the expected output is good enough more easily than relearning style later on.

Just as a deeper sense for style can be hard to grasp and teach, style is also difficult to grade. 
In computer science courses at the undergraduate level, it is standard for coding assignments to be put through a test suite for correctness, then given to Teaching Assistants and human graders to evaluate on style. 
These human graders must be trained, paid for their work, and also often create a bottleneck for assignment turnaround time. 

A simple solution is using automatic graders that handle style. 
Prior work shows humans graders are inconsistent between each other and also compared to automatic graders, which can be considered to uphold the style grading criteria exactly. 
Humans can accidentally miss violations of a style guideline, whereas automatic inspections are perfectly consistent \cite{perretta_2019}. 
Another study looking style in the context of code reviews found that patch authors rarely repeat automatically detected issues, and more frequently repeat manually detected issues in their future patch submissions. 
This implies that manual review by a human is still necessary, at least at the complexity of production-level code. 
However, it should be noted that using an automatic checker allows fewer well-trained human graders to focus on evaluating aspects that cannot yet be automatically evaluated clearly \cite{ueda_2018}.

Therefore, this project will implement an efficient style checker that utilizes autonomous checking to streamline the grading process for manual graders.  


\section{Technical Background}
\subsection{Pylint}
In order to check style, each file in a submission must be parsed by a parser and evaluated based on predefined style criteria. 
This project will use Pylint as a starting point for the checking functionalities of the web app. 
Pylint is an open source, general linter for Python, but is also often used in CS classrooms to grade student code. 
Pylint already has many features out-of-the-box that make it an appealing tool for instructors and graders: its good at enforcing generally accepted Python convention, provides recommendations for improving code, is available for students to use on their local machines before submitting, and outputs an overall score out of 10 convenient for grading. Additionally, I chose to use Pylint as the foundation for this project because unlike other linters for Python, it checks for stylistic compliance instead of just logical errors, and is one of the most critical linters. 

But as Pylint is general-purpose, it is not perfectly suited for the task of teaching nor grading style. 
For grading, student submissions graded by human instructors following predefined rubrics and submissions graded by Pylint, Pylint scores have been found to be lower than those provided by instructors \cite{dasgupta_2017}. 
Pylint is also not a suitable tool for a student to learn why their code is in poor style because feedback given by Pylint are brief and not meant to be explanatory (fig. 1). 
Furthermore, although Pylint is a widely used tool in the Python community, some users dislike that it flags too many unuseful warnings among the feedback. 
As the documentation comments, Pylint “may warn you about things that you have conscientiously done,” and “may be too verbose with warnings” \cite{pylint_documentation_2022}.

Part of the implementation of this project will include building either a plugin or wrapper around Pylint that makes improves user experience for teachers and students that only want to use a specific subset of Pylint's checking and hinting capabilities appropriate for the level of the course, compared to using a config file on Pylint directly. 

% something weirds going on with this formatting
\begin{figure}
\begin{lstlisting}
pylint_example.py 15: Line too long (120/100) (line-too-long)
\end{lstlisting}
	\centering
	\caption{A typical error message from Pylint.}
\end{figure}

\subsection{Web Stack}
This project will use React.js for the frontend, Express.js and Node.js for backend, and MySQL for its database. 
This combination of web frameworks is popular for dynamic web applications, which will be utilized in the educational component of this project to better tailor the user interface for a specific students' needs. 
MySQL was also chosen rather than non-relational databases like MongoDB to capture the complex nature of the entities invovled. 
For example, one student may be a part of many courses and have many submissions for each assignment, with each submission receiving exactly one grade.


\section{Prior Work}
Other tools that aim to teach coding style include AutoStyle, a style tutor that offers adaptive, real-time style feedback and hints \cite{moghadam_2015}. 
When compared to a tutor that only offered the end score of a submission’s style, both helped students improve the style of their code, but students were shown to improve their recognition of good style only with AutoStyle. 
However, students still struggled to implement style improvements even with the specific recommendations \cite{wiese_2017}. 
Additionally, AutoStyle’s generated hints require a large corpus of several hundred previous submissions in order to capture variations in style between submissions. 
Thus, AutoStyle may not be appropriate for new or smaller classes without such a large corpus on-hand. 
In contrast, this application aims to be ready out-of-the-box, for teachers to use.

Another approach to help student discern good and bad style is the software Ugly Code.
Ugly Code's approach to illustrating style is presenting students with good code, then transforming its style to be ugly in various specified dimensions \cite{mcmaster_2013}. 
In this way, Ugly Code aims to deepen students' understanding of style by expanding their exposure to style in novel ways. 
Unlike Ugly Code, this project will adhere to the convention of teaching style through correcting bad code to be good. 
But, it is similar to Ugly Code in that it exposes students to novel style approaches by having to correct their peer's submission. 

\section{Methods}
In order to inform the needs of my target audience, I will conduct a series of user interviews. First, I will gather participants that are professors teaching undergraduate-level CS courses at Occidental College with a goal of n >= 5. Then, I will schedule a 30 minute video conference with them in order to collect their opinions on a series of 9 questions that cover the theoretical role of code style, teaching style, and grading style (fig. 2). In the questions related to the specific courses they teach, participants will be asked to speak about the most recent semester that they taught at least one course where students were required to code. After conducting all of the interviews, I will analyze the transcripts of our conversations for common themes. In particular, questions 5 and 9 will be very useful in determining the specific features of the final web app. Several of the other questions will also reveal the context in which this web app would be used.

\begin{figure}
\begin{lstlisting}
1. How would you define good code style?
2. What role, if any, does code style play for students in each of your courses?
3. What is your approach to teaching code style in your classes and why do you use it?
4. Have you used any other approaches or products in the past to teach style? If so, what did you think of each approach or why not?
5. If someone were to create your ideal product to teach style, what functionality would you expect it to have?
6. Please give me a general description of the assignments you give to your students in each of your courses.
7. What is your approach to grading code style in your classes and why do you use it?
8. Have you used any other approaches or products in the past to grade style? If so, what did you think of each approach or why not?
9. If someone were to create your ideal product to grade style, what functionality would you expect it to have?
\end{lstlisting}
	\centering
	\caption{didn't have time to figure out this formatting}
\end{figure}

On the student side, I will use Pylint to detect the logical and stylistic errors in their code. In order to turn each of the students’ errors into opportunities to learn style, I will use Plerr, a curated list that correlates Pylint error codes with reasoning and examples of erroneous and correct code \cite{plerr}. With the combination of Pylint, Plerr, and a intuitive UI, I will build a platform for students to check the code style of their assignments, learn why their style was incorrect, and develop their own internal sense of code style beyond that assignment. 

\section{Evaluation}
In order to evaluate this project, I will return to the user interview participants to evaluate their experience using the product. In particular, I would want to ask how likely they would be to use this web app in place of their current code style teaching or grading system. It would also be important to test the usability of the product as participants interact with it. Similar to the evaluative methods of \cite{moghadam_2015} for their style grading product, it could also be interesting to compare the grade outputted by my product compared to past assignments graded by teacher’s assistants. To evaluate the student experience using and learning from the product, I could also conduct surveys or interviews that would also measure usability, as well as how effective the product is at explaining and teaching style. 

\printbibliography 

\end{document}
